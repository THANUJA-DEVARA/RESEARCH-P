{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNd68FLAIihKc1fBcQc96Bi",
      "include_colab_link": True
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b821eb70fee443dbabd837aa0a68273c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": None,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7de0bef4b37e4002a2b77cbab7b65dea",
              "IPY_MODEL_0fce369e100548418c52338d2762a373",
              "IPY_MODEL_b48512e37cd14bf9bf8314b92659d582"
            ],
            "layout": "IPY_MODEL_4bfd9d1ddace4d29919275d57d696677"
          }
        },
        "7de0bef4b37e4002a2b77cbab7b65dea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": None,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": None,
            "layout": "IPY_MODEL_e7130b19147a4fb8a39d962b3a813220",
            "placeholder": "​",
            "style": "IPY_MODEL_154106f87f204e908bc7be0e45621e25",
            "value": "Map: 100%"
          }
        },
        "0fce369e100548418c52338d2762a373": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": None,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": None,
            "layout": "IPY_MODEL_8f3ef9ad05c84fdc8b3905823f3f2bc9",
            "max": 7,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d80648ff66fd4bababe7f13c50ac0686",
            "value": 7
          }
        },
        "b48512e37cd14bf9bf8314b92659d582": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": None,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": None,
            "layout": "IPY_MODEL_bf927e971d694d7a9d1a49888ac57bc4",
            "placeholder": "​",
            "style": "IPY_MODEL_967f81dbd4c744aaad0e0015343d53c6",
            "value": " 7/7 [00:00&lt;00:00, 343.45 examples/s]"
          }
        },
        "4bfd9d1ddace4d29919275d57d696677": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": None,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": None,
            "align_items": None,
            "align_self": None,
            "border": None,
            "bottom": None,
            "display": None,
            "flex": None,
            "flex_flow": None,
            "grid_area": None,
            "grid_auto_columns": None,
            "grid_auto_flow": None,
            "grid_auto_rows": None,
            "grid_column": None,
            "grid_gap": None,
            "grid_row": None,
            "grid_template_areas": None,
            "grid_template_columns": None,
            "grid_template_rows": None,
           "height": None,
            "justify_content": None,
            "justify_items": None,
            "left": None,
            "margin": None,
            "max_height": None,
            "max_width": None,
            "min_height": None,
            "min_width": None,
            "object_fit": None,
            "object_position": None,
            "order": None,
            "overflow": None,
            "overflow_x": None,
            "overflow_y": None,
            "padding": None,
            "right": None,
            "top": None,
            "visibility": None,
            "width": None
          }
        },
        "e7130b19147a4fb8a39d962b3a813220": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": None,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
              "align_content": None,
    "align_items": None,
    "align_self": None,
    "border": None,
    "bottom": None,
    "display": None,
    "flex": None,
    "flex_flow": None,
    "grid_area": None,
    "grid_auto_columns": None,
    "grid_auto_flow": None,
    "grid_auto_rows": None,
    "grid_column": None,
    "grid_gap": None,
    "grid_row": None,
    "grid_template_areas": None,
    "grid_template_columns": None,
    "grid_template_rows": None,
    "height": None,
    "justify_content": None,
    "justify_items": None,
    "left": None,
    "margin": None,
    "max_height": None,
    "max_width": None,
    "min_height": None,
    "min_width": None,
    "object_fit": None,
    "object_position": None,
    "order": None,
    "overflow": None,
    "overflow_x": None,
    "overflow_y": None,
    "padding": None,
    "right": None,
    "top": None,
    "visibility": None,
    "width": None
},
"154106f87f204e908bc7be0e45621e25": {
    "model_module": "@jupyter-widgets/controls",
    "model_name": "DescriptionStyleModel",
    "model_module_version": "1.5.0",
    "state": {
        "_model_module": "@jupyter-widgets/controls",
        "_model_module_version": "1.5.0",
        "_model_name": "DescriptionStyleModel",
        "_view_count": None,
        "_view_module": "@jupyter-widgets/base",
        "_view_module_version": "1.2.0",
        "_view_name": "StyleView",
        "description_width": ""
    }
},
"8f3ef9ad05c84fdc8b3905823f3f2bc9": {
    "model_module": "@jupyter-widgets/base",
    "model_name": "LayoutModel",
    "model_module_version": "1.2.0",
    "state": {
        "_model_module": "@jupyter-widgets/base",
        "_model_module_version": "1.2.0",
        "_model_name": "LayoutModel",
        "_view_count": None,
        "_view_module": "@jupyter-widgets/base",
        "_view_module_version": "1.2.0",
        "_view_name": "LayoutView",
        "align_content": None,
        "align_items": None,
        "align_self": None,
        "border": None,
        "bottom": None,
        "display": None,
        "flex": None,
        "flex_flow": None,
        "grid_area": None,
        "grid_auto_columns": None,
        "grid_auto_flow": None,
        "grid_auto_rows": None,
        "grid_column": None,
        "grid_gap": None,
        "grid_row": None,
        "grid_template_areas": None,
        "grid_template_columns": None,
        "grid_template_rows": None,
        "height": None,
        "justify_content": None,
        "justify_items": None,
        "left": None,
        "margin": None,
        "max_height": None,
        "max_width": None,
        "min_height": None,
        "min_width": None,
        "object_fit": None,
        "object_position": None,
        "order": None,
        "overflow": None,
        "overflow_x": None,
        "overflow_y": None,
        "padding": None,
        "right": None,
        "top": None,
        "visibility": None,
        "width": None
    }
},
"d80648ff66fd4bababe7f13c50ac0686": {
    "model_module": "@jupyter-widgets/controls",
    "model_name": "ProgressStyleModel",
    "model_module_version": "1.5.0",
    "state": {
        "_model_module": "@jupyter-widgets/controls",
        "_model_module_version": "1.5.0",
        "_model_name": "ProgressStyleModel",
        "_view_count": None,
        "_view_module": "@jupyter-widgets/base",
        "_view_module_version": "1.2.0",
        "_view_name": "StyleView",
        "bar_color": None,
        "description_width": ""
    }
},
"bf927e971d694d7a9d1a49888ac57bc4": {
    "model_module": "@jupyter-widgets/base",
    "model_name": "LayoutModel",
    "model_module_version": "1.2.0",
    "state": {
        "_model_module": "@jupyter-widgets/base",
        "_model_module_version": "1.2.0",
        "_model_name": "LayoutModel",
        "_view_count": None,
        "_view_module": "@jupyter-widgets/base",
        "_view_module_version": "1.2.0",
        "_view_name": "LayoutView",
        "align_content": None,
        "align_items": None,
        "align_self": None,
        "border": None,
        "bottom": None,
        "display": None,
        "flex": None,
        "flex_flow": None,
        "grid_area": None,
        "grid_auto_columns": None,
        "grid_auto_flow": None,
        "grid_auto_rows": None,
        "grid_column": None,
        "grid_gap": None,
        "grid_row": None,
        "grid_template_areas": None,
        "grid_template_columns": None,
        "grid_template_rows": None,
        "height": None,
        "justify_content": None,
        "justify_items": None,
        "left": None,
        "margin": None,
        "max_height": None,
        "max_width": None,
        "min_height": None,
        "min_width": None,
        "object_fit": None,
        "object_position": None,
        "order": None,
        "overflow": None,
        "overflow_x": None,
        "overflow_y": None,
        "padding": None,
        "right": None,
        "top": None,
        "visibility": None,
        "width": None
          }
        },
        "967f81dbd4c744aaad0e0015343d53c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": None,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/THANUJA-DEVARA/RESEARCH-P/blob/main/app_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "XSi_en5NhzGN",
        "outputId": "42f2039b-833b-42f7-bddd-b64414c93dbd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-270d0f29-a191-4064-8850-35801a616b2f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-270d0f29-a191-4064-8850-35801a616b2f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving daa_os_gpt1-main.zip to daa_os_gpt1-main.zip\n",
            "User uploaded file \"daa_os_gpt1-main.zip\" with length 1434 bytes\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: my code is written down please i want to train the model addthe code fortraining the model\n",
        "\n",
        "!pip install transformers datasets\n",
        "!pip install accelerate -U\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "# Placeholder: Load and preprocess your data here\n",
        "# This is a crucial step that depends on the file types you uploaded.\n",
        "# You need to parse the text from your files and structure it in a way\n",
        "# suitable for Question Answering (e.g., SQuAD format).\n",
        "\n",
        "# For demonstration purposes, let's assume you have your data structured\n",
        "# into a list of dictionaries, where each dictionary represents a context\n",
        "# and associated question-answer pairs.\n",
        "# Example structure (you'll need to replace this with your actual loaded data):\n",
        "# data = [\n",
        "#     {\n",
        "#         'context': 'Your context text from your document.',\n",
        "#         'qas': [\n",
        "#             {\n",
        "#                 'id': 'unique_qa_id_1',\n",
        "#                 'question': 'A question about the context?',\n",
        "#                 'answers': [{'answer_start': 10, 'text': 'The answer'}]\n",
        "#             }\n",
        "#         ]\n",
        "#     },\n",
        "#     # Add more contexts and Q&A pairs\n",
        "# ]\n",
        "\n",
        "# Assuming you have loaded and processed your data into a list named 'processed_data'\n",
        "# For this example, we'll create a dummy dataset structure similar to SQuAD\n",
        "processed_data = [] # Replace with your actual processed data\n",
        "\n",
        "# Example of how your processed_data might look like if you manually\n",
        "# extracted contexts and Q&A pairs from your files:\n",
        "# processed_data = [\n",
        "#     {\n",
        "#         'context': 'Photosynthesis is a process used by plants, algae and cyanobacteria to convert light energy into chemical energy, through a process using sunlight, water and carbon dioxide.',\n",
        "#         'qas': [\n",
        "#             {\n",
        "#                 'id': 'dummy_qa_1',\n",
        "#                 'question': 'What is photosynthesis?',\n",
        "#                 'answers': [{'answer_start': 0, 'text': 'a process used by plants, algae and cyanobacteria to convert light energy into chemical energy'}]\n",
        "#             },\n",
        "#              {\n",
        "#                 'id': 'dummy_qa_2',\n",
        "#                 'question': 'What do plants use for photosynthesis?',\n",
        "#                 'answers': [{'answer_start': 151, 'text': 'sunlight, water and carbon dioxide'}]\n",
        "#             }\n",
        "#         ]\n",
        "#     },\n",
        "#     {\n",
        "#         'context': 'The capital of France is Paris. Paris is known for the Eiffel Tower and the Louvre Museum.',\n",
        "#         'qas': [\n",
        "#             {\n",
        "#                 'id': 'dummy_qa_3',\n",
        "#                 'question': 'What is the capital of France?',\n",
        "#                 'answers': [{'answer_start': 19, 'text': 'Paris'}]\n",
        "#             }\n",
        "#         ]\n",
        "#     }\n",
        "# ]\n",
        "\n",
        "# IMPORTANT: Replace the above dummy_data with the data loaded and processed\n",
        "# from your uploaded files. The structure needs to be similar to SQuAD.\n",
        "# You will need to write code to read your specific file types (PDF, text, etc.)\n",
        "# and extract contexts and potential question-answer pairs. This is the most\n",
        "# challenging and dependent part.\n",
        "\n",
        "# If your data is not in Q&A format, you might need a different model\n",
        "# (e.g., for text generation or summarization) and a different training approach.\n",
        "# Assuming you are going with a Q&A approach and have processed your data:\n",
        "\n",
        "# Convert your processed data into a Hugging Face Dataset object\n",
        "# You need to flatten the structure if you have multiple qas per context\n",
        "flattened_data = []\n",
        "for item in processed_data:\n",
        "    context = item['context']\n",
        "    for qa in item['qas']:\n",
        "        flattened_data.append({\n",
        "            'id': qa['id'],\n",
        "            'context': context,\n",
        "            'question': qa['question'],\n",
        "            'answers': qa['answers']\n",
        "        })\n",
        "\n",
        "if not flattened_data:\n",
        "    print(\"No processed data found. Please ensure your data preparation step correctly populates 'processed_data'. Model training cannot proceed.\")\n",
        "else:\n",
        "    dataset = Dataset.from_list(flattened_data)\n",
        "\n",
        "    # Choose a pre-trained model and tokenizer\n",
        "    model_name = \"distilbert-base-uncased\" # You can choose other models like \"bert-base-uncased\", \"roberta-base\", etc.\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "\n",
        "    # Preprocess the dataset\n",
        "    def preprocess_function(examples):\n",
        "        questions = [q.strip() for q in examples[\"question\"]]\n",
        "        inputs = tokenizer(\n",
        "            questions,\n",
        "            examples[\"context\"],\n",
        "            max_length=384,\n",
        "            truncation=\"only_second\",\n",
        "            return_offsets_mapping=True,\n",
        "            padding=\"max_length\",\n",
        "        )\n",
        "\n",
        "        offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "        answers = examples[\"answers\"]\n",
        "        start_positions = []\n",
        "        end_positions = []\n",
        "\n",
        "        for i, offset in enumerate(offset_mapping):\n",
        "            start_char = answers[i][0][\"answer_start\"]\n",
        "            end_char = start_char + len(answers[i][0][\"text\"])\n",
        "\n",
        "            sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "            # Find the start and end of the context\n",
        "            idx = 0\n",
        "            while sequence_ids[idx] != 1:\n",
        "                idx += 1\n",
        "            context_start = idx\n",
        "            while sequence_ids[idx] == 1:\n",
        "                idx += 1\n",
        "            context_end = idx - 1\n",
        "\n",
        "            # If the answer is not fully inside the context, label it (0, 0)\n",
        "            if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
        "                start_positions.append(0)\n",
        "                end_positions.append(0)\n",
        "            else:\n",
        "                # Otherwise it's the start and end token positions\n",
        "                idx = context_start\n",
        "                while idx <= context_end and offset[idx][0] <= start_char:\n",
        "                    idx += 1\n",
        "                start_positions.append(idx - 1)\n",
        "\n",
        "                idx = context_end\n",
        "                while idx >= context_start and offset[idx][1] >= end_char:\n",
        "                    idx -= 1\n",
        "                end_positions.append(idx + 1)\n",
        "\n",
        "        inputs[\"start_positions\"] = start_positions\n",
        "        inputs[\"end_positions\"] = end_positions\n",
        "        return inputs\n",
        "\n",
        "    tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset.column_names)\n",
        "\n",
        "    # Define training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./results\",\n",
        "        evaluation_strategy=\"epoch\", # You might want to split your data for evaluation\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        num_train_epochs=3,\n",
        "        weight_decay=0.01,\n",
        "    )\n",
        "\n",
        "    # Create the Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_dataset,\n",
        "        # eval_dataset=tokenized_eval_dataset, # Add an evaluation dataset if you have one\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    print(\"\\nStarting model training...\")\n",
        "    trainer.train()\n",
        "    print(\"Model training complete.\")\n",
        "\n",
        "    # You can save the trained model\n",
        "    model_save_path = \"./trained_qa_model\"\n",
        "    trainer.save_model(model_save_path)\n",
        "    tokenizer.save_pretrained(model_save_path)\n",
        "    print(f\"Trained model saved to {model_save_path}\")\n",
        "\n",
        "    # You are now ready to move to the evaluation and deployment steps."
      ],
      "metadata": {
        "id": "t-AG75H5jWC_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1adc5f44-1847-49e9-ea76-43758092acbf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.33.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.6.15)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m118.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "No processed data found. Please ensure your data preparation step correctly populates 'processed_data'. Model training cannot proceed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PS4Fo_jBjhrd"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f40dacac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b6703a5-5fbf-4126-b13b-83cb3e281246"
      },
      "source": [
        "!pip install streamlit"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.46.1-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.0)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.24.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.44.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.6.15)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.25.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.46.1-py3-none-any.whl (10.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.46.1 watchdog-6.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load the model once using session state\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    return pipeline(\n",
        "        \"text-generation\",\n",
        "        model=\"EleutherAI/gpt-neo-1.3B\",\n",
        "        device=-1,  # Set to 0 if you have a GPU locally\n",
        "    )\n",
        "\n",
        "model = load_model()\n",
        "\n",
        "# Title and subtitle\n",
        "st.title(\"🤖 DEVOPS PERSONALIZED LEARNING USING LLM\")\n",
        "st.subheader(\"Ask anything about DEVOPS.\")\n",
        "\n",
        "# Input box\n",
        "user_input = st.text_area(\"💬 Ask a question:\", height=100)\n",
        "\n",
        "# Answer generation\n",
        "if st.button(\"Get Answer\"):\n",
        "    if user_input.strip() == \"\":\n",
        "        st.warning(\"Please enter a question.\")\n",
        "    else:\n",
        "        prompt = (\n",
        "            \"You are an expert tutor for B.Tech students in Design and Analysis of Algorithms and Operating Systems.\\n\"\n",
        "            f\"Q: {user_input}\\nA:\"\n",
        "        )\n",
        "        with st.spinner(\"Thinking...\"):\n",
        "            response = model(\n",
        "                prompt,\n",
        "                max_length=150,\n",
        "                do_sample=True,\n",
        "                temperature=0.7,\n",
        "                top_p=0.95,\n",
        "                top_k=50,\n",
        "                num_return_sequences=1\n",
        "            )[0][\"generated_text\"]\n",
        "\n",
        "            answer = response.split(\"A:\")[-1].strip()\n",
        "\n",
        "            if not answer:\n",
        "                st.error(\"Hmm... couldn't generate a helpful response. Try rephrasing your question.\")\n",
        "            else:\n",
        "                st.success(\"Answer:\")\n",
        "                st.write(answer)"
      ],
      "metadata": {
        "id": "r1-o7458kzGR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84036361-b7fb-4add-a0c5-5725c9409a0f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-07-02 12:30:45.263 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-02 12:30:45.264 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-02 12:30:45.265 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-02 12:30:45.266 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-02 12:30:45.267 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-02 12:30:45.267 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-02 12:30:45.268 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-02 12:30:45.269 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-02 12:30:45.270 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-02 12:30:45.271 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-02 12:30:45.272 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-02 12:30:45.272 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-02 12:30:45.273 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-02 12:30:45.274 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-02 12:30:45.275 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-02 12:30:45.276 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-02 12:30:45.276 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-02 12:30:45.277 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-02 12:30:45.278 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf"
      ],
      "metadata": {
        "id": "BjVwe3Dxo03l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fbdd876-ca11-4f17-ee9d-4085ee0930a6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.11/dist-packages (1.26.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz # PyMuPDF\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Extracts text from a PDF file.\"\"\"\n",
        "    text = \"\"\n",
        "    try:\n",
        "        with fitz.open(pdf_path) as doc:\n",
        "            for page in doc:\n",
        "                text += page.get_text()\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from {pdf_path}: {e}\")\n",
        "    return text\n",
        "\n",
        "# Placeholder for where you will call the extraction function for your uploaded PDFs\n",
        "# You'll need to list the PDF files from your extracted zip folder and loop through them.\n",
        "\n",
        "# Example (assuming your PDFs are in the 'uploaded_data' folder and end with .pdf):\n",
        "pdf_texts = {}\n",
        "extracted_folder = \"uploaded_data\" # Make sure this matches the folder name in the unzip step\n",
        "\n",
        "import os\n",
        "\n",
        "for root, _, files in os.walk(extracted_folder):\n",
        "    for file in files:\n",
        "        if file.lower().endswith(\".pdf\"):\n",
        "            pdf_path = os.path.join(root, file)\n",
        "            print(f\"Extracting text from {pdf_path}...\")\n",
        "            extracted_text = extract_text_from_pdf(pdf_path)\n",
        "            if extracted_text:\n",
        "                pdf_texts[file] = extracted_text\n",
        "                print(f\"Successfully extracted text from {file}\")\n",
        "            else:\n",
        "                print(f\"No text extracted from {file}\")\n",
        "\n",
        "# Now 'pdf_texts' is a dictionary where keys are PDF filenames and values are the extracted text.\n",
        "# The next crucial step is to process this raw text into the desired Question Answering format.\n",
        "# This part is highly dependent on the structure and content of your notes and will likely require\n",
        "# significant manual effort or more advanced NLP techniques to automate.\n",
        "print(\"\\nText extraction complete. The extracted text is stored in the 'pdf_texts' dictionary.\")\n",
        "print(\"The next step is to structure this text into Question-Answering pairs for training.\")"
      ],
      "metadata": {
        "id": "VSOgFLQepI5H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c82beb1-159b-4377-de3f-533dc4ae9eda"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Text extraction complete. The extracted text is stored in the 'pdf_texts' dictionary.\n",
            "The next step is to structure this text into Question-Answering pairs for training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: give code for structure this text into Question-Answering pairs for training\n",
        "\n",
        "def create_qa_pairs_from_text(text):\n",
        "    \"\"\"\n",
        "    Attempts to structure raw text into Question-Answering pairs.\n",
        "    This is a highly simplistic example. Real-world data will likely require\n",
        "    more sophisticated techniques (e.g., span extraction, summarization,\n",
        "    or manual annotation) to create good Q&A pairs.\n",
        "\n",
        "    Args:\n",
        "        text (str): The raw text extracted from a document.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of dictionaries, each containing 'context' and 'qas'.\n",
        "              Similar to the SQuAD format.\n",
        "    \"\"\"\n",
        "    qa_data = []\n",
        "    # Split text into potential contexts (e.g., paragraphs or sections)\n",
        "    # A simple split by double newline might work for some texts\n",
        "    contexts = text.split('\\n\\n')\n",
        "\n",
        "    for i, context in enumerate(contexts):\n",
        "        if not context.strip():\n",
        "            continue # Skip empty contexts\n",
        "\n",
        "        # This is where you would ideally generate questions and find answers\n",
        "        # For this basic example, we'll just use the entire context and\n",
        "        # create dummy Q&A pairs or leave the 'qas' list empty if\n",
        "        # you plan to manually add them later.\n",
        "\n",
        "        # Option 1: Simple placeholder - you will add Q&A manually or with other tools\n",
        "        # This is the most common scenario for custom data.\n",
        "        qa_pairs = []\n",
        "        # Example of how you might add a dummy question (requires manual answer finding)\n",
        "        # qa_pairs.append({\n",
        "        #     'id': f'doc_part_{i}_qa_1',\n",
        "        #     'question': 'What is this section about?',\n",
        "        #     'answers': [] # You need to fill this manually or with extraction\n",
        "        # })\n",
        "\n",
        "        # Option 2: Very basic (and likely poor quality) automated extraction\n",
        "        # This is highly unlikely to produce good results for complex text.\n",
        "        # For demonstration: Let's try to find simple \"What is...\" sentences.\n",
        "        # import re\n",
        "        # potential_questions = re.findall(r'What is.*?[\\.\\?]', context)\n",
        "        # for j, q in enumerate(potential_questions):\n",
        "        #     # Finding a reliable answer span programmatically is very hard\n",
        "        #     # This part usually requires sophisticated methods or manual work.\n",
        "        #     # We'll skip adding answers here for simplicity.\n",
        "        #     qa_pairs.append({\n",
        "        #         'id': f'doc_part_{i}_qa_{j}',\n",
        "        #         'question': q.strip(),\n",
        "        #         'answers': [] # Requires answer span extraction\n",
        "        #     })\n",
        "\n",
        "\n",
        "        # For this example, let's just create context entries without Q&A\n",
        "        # assuming you will add the Q&A later. Or if your source text\n",
        "        # already has Q&A structure, you need to parse that structure here.\n",
        "\n",
        "        qa_data.append({\n",
        "            'context': context.strip(),\n",
        "            'qas': [] # Placeholder for actual Question-Answer pairs\n",
        "                     # You need to populate this list based on your data source\n",
        "                     # Each item in 'qas' should be like:\n",
        "                     # {'id': 'unique_id', 'question': 'Your question?', 'answers': [{'answer_start': start_idx, 'text': 'The answer text'}]}\n",
        "        })\n",
        "\n",
        "    return qa_data\n",
        "\n",
        "# Process the extracted text from PDFs\n",
        "processed_data = []\n",
        "for filename, text in pdf_texts.items():\n",
        "    print(f\"Structuring text from {filename} into Q&A format...\")\n",
        "    # Here you call the function to process the text from each file\n",
        "    # and potentially add more sophisticated logic depending on your file structure.\n",
        "    # For this basic example, we use the simple function above.\n",
        "    qa_structured_data = create_qa_pairs_from_text(text)\n",
        "\n",
        "    # Add the structured data from this file to the main list\n",
        "    # You might want to add a source identifier if needed\n",
        "    processed_data.extend(qa_structured_data)\n",
        "\n",
        "print(f\"\\nFinished structuring text. Created {len(processed_data)} potential context entries.\")\n",
        "print(\"Please inspect 'processed_data' and populate the 'qas' lists with actual questions and answers.\")\n",
        "# print(\"Example of processed_data (first entry):\")\n",
        "# if processed_data:\n",
        "#    print(processed_data[0])\n",
        "\n",
        "# After this step, 'processed_data' contains contexts extracted from your documents.\n",
        "# You MUST NOW manually or programmatically add the 'qas' (Question-Answer pairs)\n",
        "# to each context dictionary in 'processed_data'. This is the most critical part\n",
        "# of preparing your data for Question Answering model training.\n",
        "\n",
        "# Once 'processed_data' has the correct SQuAD-like structure with contexts and qas,\n",
        "# the subsequent code for converting to Dataset, tokenizing, and training can be executed.\n",
        "\n",
        "# Example of how to add a manual Q&A pair to a context (if you inspect your data):\n",
        "# if processed_data:\n",
        "#    first_context = processed_data[0]['context']\n",
        "#    # Find the start index of an answer within the context\n",
        "#    answer_text = \"Photosynthesis\"\n",
        "#    start_idx = first_context.find(answer_text)\n",
        "#    if start_idx != -1:\n",
        "#        processed_data[0]['qas'].append({\n",
        "#            'id': 'manual_qa_1',\n",
        "#            'question': 'What is the key process?',\n",
        "#            'answers': [{'answer_start': start_idx, 'text': answer_text}]\n",
        "#        })\n",
        "\n",
        "# The subsequent code in your original snippet assumes 'processed_data'\n",
        "# is populated with the correct structure, including 'qas' lists.\n",
        "# Ensure you populate 'processed_data' correctly before running the training part.\n"
      ],
      "metadata": {
        "id": "j9sklzMkpZV9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4968fb2b-a873-43c7-c5a8-788d44e624f3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Finished structuring text. Created 0 potential context entries.\n",
            "Please inspect 'processed_data' and populate the 'qas' lists with actual questions and answers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: code for 'processed_data' and populate the 'qas' lists with actual questions and answers\n",
        "\n",
        "processed_data = [\n",
        "    {\n",
        "        'context': 'Photosynthesis is a process used by plants, algae and cyanobacteria to convert light energy into chemical energy, through a process using sunlight, water and carbon dioxide.',\n",
        "        'qas': [\n",
        "            {\n",
        "                'id': 'dummy_qa_1',\n",
        "                'question': 'What is photosynthesis?',\n",
        "                'answers': [{'answer_start': 0, 'text': 'a process used by plants, algae and cyanobacteria to convert light energy into chemical energy'}]\n",
        "            },\n",
        "             {\n",
        "                'id': 'dummy_qa_2',\n",
        "                'question': 'What do plants use for photosynthesis?',\n",
        "                'answers': [{'answer_start': 151, 'text': 'sunlight, water and carbon dioxide'}]\n",
        "            }\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        'context': 'The capital of France is Paris. Paris is known for the Eiffel Tower and the Louvre Museum.',\n",
        "        'qas': [\n",
        "            {\n",
        "                'id': 'dummy_qa_3',\n",
        "                'question': 'What is the capital of France?',\n",
        "                'answers': [{'answer_start': 19, 'text': 'Paris'}]\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "Mdw26z1Npkno"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8aZ2Oi__pxud",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc118cb0-0443-4814-9d12-6617547aa4ce"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing the trained Question Answering model...\n",
            "\n",
            "Context: Photosynthesis is a process used by plants, algae and cyanobacteria to convert light energy into chemical energy, through a process using sunlight, water and carbon dioxide.\n",
            "Question: What is photosynthesis?\n",
            "Answer: Photosynthesis is a process used\n",
            "Score: 0.0014\n",
            "\n",
            "Context: Photosynthesis is a process used by plants, algae and cyanobacteria to convert light energy into chemical energy, through a process using sunlight, water and carbon dioxide.\n",
            "Question: What do plants use for photosynthesis?\n",
            "Answer: cyanobacteria to convert light energy into chemical energy, through a\n",
            "Score: 0.0014\n",
            "\n",
            "Context: The capital of France is Paris. Paris is known for the Eiffel Tower and the Louvre Museum.\n",
            "Question: What is in Paris?\n",
            "Answer: Louvre Museum\n",
            "Score: 0.0035\n",
            "\n",
            "Model testing complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G-mhET1Up_3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8a67b06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1ba4e39-8591-4e65-cdb5-2f375bedc77a"
      },
      "source": [
        "!pip install transformers datasets torch accelerate pymupdf"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\n",
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.11/dist-packages (1.26.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: give code without error\n",
        "\n",
        "from google.colab import files\n",
        "import io\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
        "from datasets import load_dataset, Dataset\n",
        "import fitz # PyMuPDF\n",
        "import os\n",
        "import zipfile # Import zipfile\n",
        "\n",
        "# Remove Streamlit imports and code sections as Streamlit is not directly runnable in Colab notebooks this way.\n",
        "# from transformers import pipeline # Pipeline is used later for Q&A inference\n",
        "\n",
        "# Upload files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Assuming the user uploads a single zip file containing PDFs\n",
        "uploaded_file_name = list(uploaded.keys())[0]\n",
        "extracted_folder = \"uploaded_data\"\n",
        "\n",
        "# Unzip the uploaded file\n",
        "if uploaded_file_name.lower().endswith('.zip'):\n",
        "    print(f\"Unzipping {uploaded_file_name}...\")\n",
        "    with zipfile.ZipFile(io.BytesIO(uploaded[uploaded_file_name]), 'r') as zip_ref:\n",
        "        zip_ref.extractall(extracted_folder)\n",
        "    print(f\"Successfully unzipped to {extracted_folder}\")\n",
        "else:\n",
        "    print(f\"Uploaded file '{uploaded_file_name}' is not a zip file. Please upload a zip archive.\")\n",
        "    # Exit or handle the case where a non-zip file is uploaded if necessary\n",
        "    # For this example, we'll proceed, but PDF extraction won't happen without a zip.\n",
        "\n",
        "# Install necessary libraries\n",
        "!pip install transformers datasets torch accelerate pymupdf\n",
        "\n",
        "# Function to extract text from PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Extracts text from a PDF file.\"\"\"\n",
        "    text = \"\"\n",
        "    try:\n",
        "        with fitz.open(pdf_path) as doc:\n",
        "            for page in doc:\n",
        "                text += page.get_text()\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from {pdf_path}: {e}\")\n",
        "    return text\n",
        "\n",
        "# Extract text from PDFs in the unzipped folder\n",
        "pdf_texts = {}\n",
        "if os.path.exists(extracted_folder):\n",
        "    for root, _, files in os.walk(extracted_folder):\n",
        "        for file in files:\n",
        "            if file.lower().endswith(\".pdf\"):\n",
        "                pdf_path = os.path.join(root, file)\n",
        "                print(f\"Extracting text from {pdf_path}...\")\n",
        "                extracted_text = extract_text_from_pdf(pdf_path)\n",
        "                if extracted_text:\n",
        "                    pdf_texts[file] = extracted_text\n",
        "                    print(f\"Successfully extracted text from {file}\")\n",
        "                else:\n",
        "                    print(f\"No text extracted from {file}\")\n",
        "else:\n",
        "    print(f\"Extracted folder '{extracted_folder}' not found. Skipping PDF extraction.\")\n",
        "\n",
        "# Assuming 'pdf_texts' contains the extracted text from your PDFs\n",
        "# The goal is to convert this raw text into a format suitable for training a Q&A model.\n",
        "# This is a critical step and depends heavily on the nature of your notes.\n",
        "\n",
        "# For demonstration, let's use the dummy data structure if processed_data is empty\n",
        "processed_data = [] # Replace with your actual processed data based on pdf_texts\n",
        "\n",
        "# You need to write code here to populate `processed_data` by processing `pdf_texts`.\n",
        "# This is the most complex part and depends entirely on how you want to structure your notes\n",
        "# into context-question-answer triples.\n",
        "\n",
        "# Example of how you might process extracted text (highly simplified):\n",
        "# Iterate through pdf_texts and potentially split into smaller contexts\n",
        "# for filename, text in pdf_texts.items():\n",
        "#     # Split text into paragraphs or sections\n",
        "#     sections = text.split('\\n\\n') # Example split by double newline\n",
        "#     for section in sections:\n",
        "#         if len(section) > 100: # Use sections longer than a minimum length as context\n",
        "#             # Here you would need to generate or extract questions and answers for this section\n",
        "#             # This is non-trivial and often requires manual annotation or advanced techniques.\n",
        "#             # For example, if you had question-answer pairs explicitly written in your notes:\n",
        "#             # if \"Q:\" in section and \"A:\" in section:\n",
        "#             #    q_start = section.find(\"Q:\") + 2\n",
        "#             #    a_start = section.find(\"A:\")\n",
        "#             #    question = section[q_start:a_start].strip()\n",
        "#             #    answer_text = section[a_start + 2:].strip()\n",
        "#             #    # You would need to find the start_char of the answer within the 'context' (section)\n",
        "#             #    answer_start_char = section.find(answer_text) # This can be complex with partial matches\n",
        "#             #    if answer_start_char != -1:\n",
        "#             #         processed_data.append({\n",
        "#             #             'context': section,\n",
        "#             #             'qas': [{'id': f'{filename}_{len(processed_data)}', 'question': question, 'answers': [{'answer_start': answer_start_char, 'text': answer_text}]}]\n",
        "#             #         })\n",
        "\n",
        "\n",
        "# --- Start Dummy Data (Remove this section if you are using real data processing) ---\n",
        "if not processed_data:\n",
        "    print(\"\\n'processed_data' is empty. Using dummy data for demonstration purposes.\")\n",
        "    processed_data = [\n",
        "        {\n",
        "            'context': 'Photosynthesis is a process used by plants, algae and cyanobacteria to convert light energy into chemical energy, through a process using sunlight, water and carbon dioxide.',\n",
        "            'qas': [\n",
        "                {\n",
        "                    'id': 'dummy_qa_1',\n",
        "                    'question': 'What is photosynthesis?',\n",
        "                    'answers': [{'answer_start': 0, 'text': 'a process used by plants, algae and cyanobacteria to convert light energy into chemical energy'}]\n",
        "                },\n",
        "                 {\n",
        "                    'id': 'dummy_qa_2',\n",
        "                    'question': 'What do plants use for photosynthesis?',\n",
        "                    'answers': [{'answer_start': 151, 'text': 'sunlight, water and carbon dioxide'}]\n",
        "                }\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            'context': 'The capital of France is Paris. Paris is known for the Eiffel Tower and the Louvre Museum.',\n",
        "            'qas': [\n",
        "                {\n",
        "                    'id': 'dummy_qa_3',\n",
        "                    'question': 'What is the capital of France?',\n",
        "                    'answers': [{'answer_start': 19, 'text': 'Paris'}]\n",
        "                }\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            'context': 'Merge sort is a divide and conquer algorithm. It works by recursively dividing the input array into two halves, calling itself for the two halves, and then merging the two sorted halves.',\n",
        "            'qas': [\n",
        "                {\n",
        "                    'id': 'dummy_qa_4',\n",
        "                    'question': 'What type of algorithm is merge sort?',\n",
        "                    'answers': [{'answer_start': 16, 'text': 'divide and conquer'}]\n",
        "                },\n",
        "                {\n",
        "                     'id': 'dummy_qa_5',\n",
        "                     'question': 'How does merge sort work?',\n",
        "                     'answers': [{'answer_start': 47, 'text': 'by recursively dividing the input array into two halves, calling itself for the two halves, and then merging the two sorted halves'}]\n",
        "                }\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            'context': 'Operating system kernels manage system resources. This includes memory management, process scheduling, and handling hardware interrupts.',\n",
        "            'qas': [\n",
        "                {\n",
        "                    'id': 'dummy_qa_6',\n",
        "                    'question': 'What is the role of an operating system kernel?',\n",
        "                    'answers': [{'answer_start': 29, 'text': 'manage system resources'}]\n",
        "                },\n",
        "                {\n",
        "                    'id': 'dummy_qa_7',\n",
        "                    'question': 'What resources do kernels manage?',\n",
        "                    'answers': [{'answer_start': 58, 'text': 'memory management, process scheduling, and handling hardware interrupts'}]\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "# --- End Dummy Data ---\n",
        "\n",
        "\n",
        "# Flatten the processed data for the Dataset object\n",
        "flattened_data = []\n",
        "for item in processed_data:\n",
        "    context = item['context']\n",
        "    for qa in item['qas']:\n",
        "        # Basic check for valid answer_start and text existence\n",
        "        if qa['answers'] and 'answer_start' in qa['answers'][0] and 'text' in qa['answers'][0]:\n",
        "             flattened_data.append({\n",
        "                 'id': qa['id'],\n",
        "                 'context': context,\n",
        "                 'question': qa['question'],\n",
        "                 'answers': qa['answers'] # answers should be a list of dicts\n",
        "             })\n",
        "        else:\n",
        "            print(f\"Skipping invalid Q&A pair with ID {qa.get('id', 'N/A')} due to missing answer info.\")\n",
        "\n",
        "\n",
        "if not flattened_data:\n",
        "    print(\"No valid processed data found after flattening. Model training cannot proceed.\")\n",
        "else:\n",
        "    try:\n",
        "        dataset = Dataset.from_list(flattened_data)\n",
        "\n",
        "        # Choose a pre-trained model and tokenizer\n",
        "        model_name = \"distilbert-base-uncased\"\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "\n",
        "        # Preprocess the dataset\n",
        "        def preprocess_function(examples):\n",
        "            questions = [q.strip() for q in examples[\"question\"]]\n",
        "            inputs = tokenizer(\n",
        "                questions,\n",
        "                examples[\"context\"],\n",
        "                max_length=384,\n",
        "                truncation=\"only_second\",\n",
        "                return_offsets_mapping=True,\n",
        "                padding=\"max_length\",\n",
        "            )\n",
        "\n",
        "            offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "            answers = examples[\"answers\"]\n",
        "            start_positions = []\n",
        "            end_positions = []\n",
        "\n",
        "            for i, offset in enumerate(offset_mapping):\n",
        "                # Handle cases with no answers or invalid answer structures\n",
        "                if not answers[i] or not isinstance(answers[i][0], dict) or 'answer_start' not in answers[i][0] or 'text' not in answers[i][0]:\n",
        "                     start_positions.append(0)\n",
        "                     end_positions.append(0)\n",
        "                     continue # Skip to the next example\n",
        "\n",
        "                start_char = answers[i][0][\"answer_start\"]\n",
        "                end_char = start_char + len(answers[i][0][\"text\"])\n",
        "\n",
        "                sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "                idx = 0\n",
        "                while idx < len(sequence_ids) and sequence_ids[idx] != 1:\n",
        "                    idx += 1\n",
        "                context_start = idx\n",
        "                while idx < len(sequence_ids) and sequence_ids[idx] == 1:\n",
        "                    idx += 1\n",
        "                context_end = idx - 1\n",
        "\n",
        "                # If the answer is not fully inside the context or the context is empty, label it (0, 0)\n",
        "                if context_start >= len(sequence_ids) or offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
        "                    start_positions.append(0)\n",
        "                    end_positions.append(0)\n",
        "                else:\n",
        "                    idx = context_start\n",
        "                    while idx <= context_end and offset[idx][0] <= start_char:\n",
        "                        idx += 1\n",
        "                    start_positions.append(idx - 1)\n",
        "\n",
        "                    idx = context_end\n",
        "                    while idx >= context_start and offset[idx][1] >= end_char:\n",
        "                        idx -= 1\n",
        "                    end_positions.append(idx + 1)\n",
        "\n",
        "            inputs[\"start_positions\"] = start_positions\n",
        "            inputs[\"end_positions\"] = end_positions\n",
        "            return inputs\n",
        "\n",
        "        tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset.column_names)\n",
        "\n",
        "        # Define training arguments\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=\"./results\",\n",
        "            evaluation_strategy=\"no\", # Set to \"epoch\" if you have an evaluation set\n",
        "            learning_rate=2e-5,\n",
        "            per_device_train_batch_size=16,\n",
        "            per_device_eval_batch_size=16,\n",
        "            num_train_epochs=3,\n",
        "            weight_decay=0.01,\n",
        "            save_strategy=\"epoch\",\n",
        "            logging_dir=\"./logs\",\n",
        "            logging_steps=10,\n",
        "        )\n",
        "\n",
        "        # Create the Trainer\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=tokenized_dataset,\n",
        "            tokenizer=tokenizer,\n",
        "        )\n",
        "\n",
        "        # Train the model\n",
        "        print(\"\\nStarting model training...\")\n",
        "        trainer.train()\n",
        "        print(\"Model training complete.\")\n",
        "\n",
        "        # Save the trained model and tokenizer\n",
        "        model_save_path = \"./trained_qa_model\"\n",
        "        trainer.save_model(model_save_path)\n",
        "        tokenizer.save_pretrained(model_save_path)\n",
        "        print(f\"Trained model saved to {model_save_path}\")\n",
        "\n",
        "        # Example of how to use the trained model for inference:\n",
        "        from transformers import pipeline # Import pipeline here\n",
        "\n",
        "        try:\n",
        "            # Load the trained model and tokenizer\n",
        "            trained_tokenizer = AutoTokenizer.from_pretrained(model_save_path)\n",
        "            trained_model = AutoModelForQuestionAnswering.from_pretrained(model_save_path)\n",
        "\n",
        "            # Create a question answering pipeline\n",
        "            qa_pipeline = pipeline(\"question-answering\", model=trained_model, tokenizer=trained_tokenizer)\n",
        "\n",
        "            print(\"\\nTrained model loaded for testing.\")\n",
        "\n",
        "            # Test with a question (replace with a question relevant to your training data)\n",
        "            if processed_data:\n",
        "                # Pick a context and question for testing\n",
        "                # Use a context and question from the processed data to ensure relevance\n",
        "                test_item = processed_data[0] # Use the first item in your processed data\n",
        "                test_context = test_item['context']\n",
        "                test_question = test_item['qas'][0]['question'] if test_item['qas'] else \"What is this about?\"\n",
        "\n",
        "                print(f\"\\nContext: {test_context}\")\n",
        "                print(f\"Question: {test_question}\")\n",
        "\n",
        "                # Get the answer using the pipeline\n",
        "                answer = qa_pipeline(question=test_question, context=test_context)\n",
        "\n",
        "                print(\"\\nAnswer:\")\n",
        "                print(f\"Text: {answer['answer']}\")\n",
        "                print(f\"Score: {answer['score']:.2f}\")\n",
        "                print(f\"Start: {answer['start']}\")\n",
        "                print(f\"End: {answer['end']}\")\n",
        "            else:\n",
        "                print(\"Cannot perform inference testing: No processed data available to pick a context/question from.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError during model inference testing: {e}\")\n",
        "            print(\"Please ensure the model was trained successfully and the 'trained_qa_model' directory exists and contains valid model files.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError during dataset creation or tokenization: {e}\")\n",
        "        print(\"Please check the 'processed_data' structure and the 'preprocess_function'.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "6QCMLV_fqMBc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b821eb70fee443dbabd837aa0a68273c",
            "7de0bef4b37e4002a2b77cbab7b65dea",
            "0fce369e100548418c52338d2762a373",
            "b48512e37cd14bf9bf8314b92659d582",
            "4bfd9d1ddace4d29919275d57d696677",
            "e7130b19147a4fb8a39d962b3a813220",
            "154106f87f204e908bc7be0e45621e25",
            "8f3ef9ad05c84fdc8b3905823f3f2bc9",
            "d80648ff66fd4bababe7f13c50ac0686",
            "bf927e971d694d7a9d1a49888ac57bc4",
            "967f81dbd4c744aaad0e0015343d53c6"
          ]
        },
        "outputId": "4fb42f80-b059-4b97-e29d-a545ec7911fa"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7057d392-6026-4bf2-b355-04acb7a15a27\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7057d392-6026-4bf2-b355-04acb7a15a27\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving java.zip to java (1).zip\n",
            "Unzipping java (1).zip...\n",
            "Successfully unzipped to uploaded_data\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\n",
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.11/dist-packages (1.26.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Extracting text from uploaded_data/java/POJ UNIT - 5.pdf...\n",
            "Successfully extracted text from POJ UNIT - 5.pdf\n",
            "Extracting text from uploaded_data/java/POJ UNIT - 2.pdf...\n",
            "Successfully extracted text from POJ UNIT - 2.pdf\n",
            "Extracting text from uploaded_data/java/POJ UNIT - 1.pdf...\n",
            "Successfully extracted text from POJ UNIT - 1.pdf\n",
            "Extracting text from uploaded_data/java/POJ UNIT - 3.pdf...\n",
            "Successfully extracted text from POJ UNIT - 3.pdf\n",
            "Extracting text from uploaded_data/java/POJ UNIT - 4.pdf...\n",
            "Successfully extracted text from POJ UNIT - 4.pdf\n",
            "\n",
            "'processed_data' is empty. Using dummy data for demonstration purposes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/7 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b821eb70fee443dbabd837aa0a68273c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Error during dataset creation or tokenization: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'\n",
            "Please check the 'processed_data' structure and the 'preprocess_function'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: check  'processed_data' structure and the 'preprocess_function'\n",
        "\n",
        "# Check the structure of processed_data\n",
        "print(\"Structure of processed_data (first item):\")\n",
        "if processed_data:\n",
        "    import json\n",
        "    print(json.dumps(processed_data[0], indent=2))\n",
        "else:\n",
        "    print(\"processed_data is empty.\")\n",
        "\n",
        "# Check the preprocess_function by processing a single example\n",
        "print(\"\\nTesting preprocess_function on a single example:\")\n",
        "if flattened_data:\n",
        "    example = flattened_data[0]\n",
        "    # Wrap the single example in a list as the function expects batched input\n",
        "    batched_example = {key: [value] for key, value in example.items()}\n",
        "    try:\n",
        "        processed_example = preprocess_function(batched_example)\n",
        "        print(\"Successfully processed a single example.\")\n",
        "        # Print some processed outputs for inspection\n",
        "        print(\"Input IDs (first 10):\", processed_example['input_ids'][0][:10])\n",
        "        print(\"Attention Mask (first 10):\", processed_example['attention_mask'][0][:10])\n",
        "        print(\"Start Position:\", processed_example['start_positions'][0])\n",
        "        print(\"End Position:\", processed_example['end_positions'][0])\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during preprocess_function test: {e}\")\n",
        "else:\n",
        "    print(\"flattened_data is empty. Cannot test preprocess_function.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8r9VESXFKDgo",
        "outputId": "46e5bb63-44c0-4f49-a652-8e69e984013a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Structure of processed_data (first item):\n",
            "{\n",
            "  \"context\": \"Photosynthesis is a process used by plants, algae and cyanobacteria to convert light energy into chemical energy, through a process using sunlight, water and carbon dioxide.\",\n",
            "  \"qas\": [\n",
            "    {\n",
            "      \"id\": \"dummy_qa_1\",\n",
            "      \"question\": \"What is photosynthesis?\",\n",
            "      \"answers\": [\n",
            "        {\n",
            "          \"answer_start\": 0,\n",
            "          \"text\": \"a process used by plants, algae and cyanobacteria to convert light energy into chemical energy\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"dummy_qa_2\",\n",
            "      \"question\": \"What do plants use for photosynthesis?\",\n",
            "      \"answers\": [\n",
            "        {\n",
            "          \"answer_start\": 151,\n",
            "          \"text\": \"sunlight, water and carbon dioxide\"\n",
            "        }\n",
            "      ]\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "\n",
            "Testing preprocess_function on a single example:\n",
            "Successfully processed a single example.\n",
            "Input IDs (first 10): [101, 2054, 2003, 7760, 6038, 25078, 1029, 102, 7760, 6038]\n",
            "Attention Mask (first 10): [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "Start Position: 8\n",
            "End Position: 29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: code for test the model\n",
        "\n",
        "# Test the trained model with a sample question\n",
        "# This assumes the model was successfully trained and saved in the previous steps\n",
        "\n",
        "try:\n",
        "    # Load the trained model and tokenizer\n",
        "    model_save_path = \"./trained_qa_model\"\n",
        "    trained_tokenizer = AutoTokenizer.from_pretrained(model_save_path)\n",
        "    trained_model = AutoModelForQuestionAnswering.from_pretrained(model_save_path)\n",
        "\n",
        "    # Create a question answering pipeline\n",
        "    qa_pipeline = pipeline(\"question-answering\", model=trained_model, tokenizer=trained_tokenizer)\n",
        "\n",
        "    print(\"\\nTrained model loaded for testing.\")\n",
        "\n",
        "    # Test with a question relevant to your training data.\n",
        "    # Pick a context and question from the processed data to ensure relevance\n",
        "    if processed_data:\n",
        "        # Use the first item in your processed data as an example\n",
        "        test_item = processed_data[0]\n",
        "        test_context = test_item['context']\n",
        "        # Use the first question for this context, if available, otherwise a generic one\n",
        "        test_question = test_item['qas'][0]['question'] if test_item.get('qas') else \"What is this text about?\"\n",
        "\n",
        "        print(f\"\\nContext used for testing: {test_context}\")\n",
        "        print(f\"Question used for testing: {test_question}\")\n",
        "\n",
        "        # Get the answer using the pipeline\n",
        "        answer = qa_pipeline(question=test_question, context=test_context)\n",
        "\n",
        "        print(\"\\nModel Prediction:\")\n",
        "        print(f\"Answer Text: {answer['answer']}\")\n",
        "        print(f\"Confidence Score: {answer['score']:.4f}\")\n",
        "        print(f\"Start Position: {answer['start']}\")\n",
        "        print(f\"End Position: {answer['end']}\")\n",
        "\n",
        "    else:\n",
        "        print(\"Cannot perform inference testing: 'processed_data' is empty or not available.\")\n",
        "        print(\"Ensure your data processing step successfully populated 'processed_data'.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nError during model inference testing: {e}\")\n",
        "    print(\"Please ensure the model was trained successfully and the 'trained_qa_model' directory exists and contains valid model files.\")\n",
        "    print(\"If training failed, the model files won't be present.\")\n",
        "\n",
        "print(\"\\nModel testing code executed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaX01mnKKM3P",
        "outputId": "08cb797d-eb33-4c7c-cc1a-b4700c023e93"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Error during model inference testing: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: './trained_qa_model'.\n",
            "Please ensure the model was trained successfully and the 'trained_qa_model' directory exists and contains valid model files.\n",
            "If training failed, the model files won't be present.\n",
            "\n",
            "Model testing code executed.\n"
          ]
        }
      ]
    }
  ]
}
}
